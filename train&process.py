# -*- coding: utf-8 -*-
"""Train&Process.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1wWY6xfi6X2YYekJRx0rG7c89MA5zg-LM
"""

#Create MP4 Video from Images

import cv2
import os
import re # For natural sorting

image_folder = '/content/UA_DATA/images/train' # !! REPLACE THIS !!

video_name = '/content/traffic_video_1000frames_24fps.mp4'

fps = 24

num_images_to_process = 1000

if not os.path.exists(image_folder):
    print(f"ERROR: Image folder not found at '{image_folder}'. Please check the path.")
else:
    # Get all image files from the folder
    images = [img for img in os.listdir(image_folder) if img.lower().endswith((".png", ".jpg", ".jpeg"))]

    if not images:
        print(f"No images found in '{image_folder}'.")
    else:
        # Sort images naturally to ensure correct order (e.g., img1, img2, ..., img10, not img1, img10, img2)
        def natural_sort_key(s):
            return [int(text) if text.isdigit() else text.lower() for text in re.split('([0-9]+)', s)]
        images.sort(key=natural_sort_key)

        # Select the desired number of images
        if len(images) > num_images_to_process:
            print(f"Found {len(images)} images, using the first {num_images_to_process} for the video.")
            images_to_use = images[:num_images_to_process]
        else:
            print(f"Found {len(images)} images, using all of them for the video.")
            images_to_use = images

        if not images_to_use:
            print("No images selected to process for the video after filtering.")
        else:
            # Determine the width and height from the first image.
            # All images in the sequence should have the same dimensions.
            first_image_path = os.path.join(image_folder, images_to_use[0])
            frame = cv2.imread(first_image_path)

            if frame is None:
                print(f"Error: Could not read the first image: {first_image_path}")
                print("Please ensure the image path is correct and the file is a valid image.")
            else:
                height, width, layers = frame.shape

                # Define the codec and create VideoWriter object
                # For .mp4 videos, 'mp4v' or 'XVID' are common codecs.
                # 'XVID' is often more widely compatible if 'mp4v' gives issues.
                fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                video = cv2.VideoWriter(video_name, fourcc, float(fps), (width, height))

                print(f"\nCreating video '{video_name}' from {len(images_to_use)} images at {fps} FPS...")
                print(f"Video dimensions will be: {width}x{height}")

                for i, image_name in enumerate(images_to_use):
                    img_path = os.path.join(image_folder, image_name)
                    current_frame = cv2.imread(img_path)
                    if current_frame is not None:
                        video.write(current_frame)
                    else:
                        print(f"Warning: Could not read image {img_path}, skipping this frame.")

                    if (i + 1) % 100 == 0: # Print progress every 100 frames
                        print(f"  Processed {i + 1}/{len(images_to_use)} frames...")

                video.release() # Release the VideoWriter object
                cv2.destroyAllWindows() # Should not be needed if not using cv2.imshow()
                print(f"\nVideo '{video_name}' created successfully!")
                print("You can find it in your Colab environment's /content/ directory.")

                # To download the video from Colab to your local machine:
                # from google.colab import files
                # files.download(video_name)

#install necessary libraries
!pip install ultralytics -q

# Training Starts Here.

from ultralytics import YOLO
model = YOLO('yolo11n.pt')

print("Starting YOLOv8 training...")
print("This will take a while. Output for each epoch will be shown below.")
print("Look for 'runs/detect/train...' folder for results after completion.")

# Start training
results = model.train(
    data='/content/ua_detrac_config.yaml',
    epochs=50,                    # As per PDF: train for at least 50 epochs
                                  # For a quick first test, you could try a smaller number like 5-10,
                                  # then run again with 50 once you confirm it's working.
    imgsz=416,                    # Image size for training (YOLO will resize images to this)
    batch=128,                      # Number of images to process at once.
                                  # If you get "Out of Memory" errors, try reducing this (e.g., 4 or 2).
                                  # If you have a lot of GPU memory, you can increase it (e.g., 16).
    name='yolov8s_ua_detrac_run1' # A name for the folder where results will be saved
)

print("YOLOv8 training has finished (or was interrupted).")
print("Results and trained model are saved in the 'runs/detect/yolov8s_ua_detrac_run1' folder (or similar name).")

#single image tester after training done

path_to_test_image_on_drive = "/content/UA_DATA/images/train/MVI_20011_img00012.jpg"

if os.path.exists(path_to_test_image_on_drive):
    print(f"\nPerforming inference on: {path_to_test_image_on_drive} with your custom model...")

    # Perform detection
    results = model(path_to_test_image_on_drive)

    # Visualize the results
    annotated_image_np = results[0].plot() # Returns BGR image
    annotated_image_rgb = cv2.cvtColor(annotated_image_np, cv2.COLOR_BGR2RGB) # Convert to RGB for matplotlib
    display_image_array(annotated_image_rgb, title=f"Detections on '{os.path.basename(path_to_test_image_on_drive)}'")

    # Access and print raw prediction data
    print(f"\n--- Raw Prediction Data for: {os.path.basename(path_to_test_image_on_drive)} ---")
    boxes = results[0].boxes
    if boxes is not None and len(boxes) > 0:
        for i in range(len(boxes)):
            xyxy = boxes.xyxy[i]
            conf = boxes.conf[i]
            cls_id = boxes.cls[i]
            class_name = model.names[int(cls_id)] # Get class name from model's mapping
            print(f"Object {i+1}: Class={class_name}, Conf={conf:.2f}, Box (xmin, ymin, xmax, ymax)={xyxy.tolist()}")
    else:
        print("No objects detected in this image by your custom model.")

else:
    print(f"Test image not found at the specified Google Drive path: {path_to_test_image_on_drive}")
    print("Please ensure the path is correct and your Google Drive is mounted.")

# Load the Trained Model and Process the Video

import cv2
import os
from ultralytics import YOLO
from IPython.display import HTML, display
from base64 import b64encode
import numpy as np
import collections # For defaultdict, which is useful here


path_to_your_trained_model = "/content/runs/detect/yolov8s_ua_detrac_run1/weights/best.pt" # e.g., yolov8_ua_detrac_corrected_run1

try:
    model = YOLO(path_to_your_trained_model)
    print(f"Custom trained model loaded successfully from {path_to_your_trained_model}")
except Exception as e:
    print(f"Error loading model: {e}")
    print("Please ensure the path to your 'best.pt' file is correct.")
    model = None


path_to_your_test_video = "/content/traffic_video_1000frames_24fps.mp4" # Or your input video
output_video_name_per_class = "/content/traffic_video_per_class_color_counted.mp4" # Changed output name


per_class_counted_track_ids = collections.defaultdict(set)
video_writer = None

CLASS_COLORS = [
    (255, 0, 0),    # Blue for class 0 (e.g., bicycle)
    (0, 255, 0),    # Green for class 1 (e.g., motorcycle)
    (0, 0, 255),    # Red for class 2 (e.g., car)
    (255, 255, 0),  # Cyan for class 3 (e.g., transporter (van))
    (255, 0, 255),  # Magenta for class 4 (e.g., bus)
    (0, 255, 255),  # Yellow for class 5 (e.g., truck (others))
    (128, 0, 0),    # Dark Blue
    (0, 128, 0),    # Dark Green
    (0, 0, 128),    # Dark Red
    (128, 128, 0),  # Teal
    (128, 0, 128),  # Purple
    (0, 128, 128)   # Olive
]

if model and os.path.exists(path_to_your_test_video):
    print(f"\nStarting per-class object tracking and counting on video: {path_to_your_test_video}...")

    results_generator = model.track(
        source=path_to_your_test_video,
        stream=True,
        persist=True,
        tracker='bytetrack.yaml',
        conf=0.3,
        iou=0.5,
        save=False # Manual drawing and saving
    )

    frame_count_processed = 0
    try:
        for result in results_generator:
            frame_to_draw = result.orig_img.copy()
            frame_height, frame_width = frame_to_draw.shape[:2]

            if video_writer is None: # Initialize VideoWriter
                fps_out = 24 # Or get from source video
                fourcc = cv2.VideoWriter_fourcc(*'mp4v')
                video_writer = cv2.VideoWriter(output_video_name_per_class, fourcc, float(fps_out), (frame_width, frame_height))
                if not video_writer.isOpened():
                    print(f"Error: Could not open VideoWriter for {output_video_name_per_class}")
                    break
                print(f"Output video: {output_video_name_per_class}, FPS: {fps_out}, Res: {frame_width}x{frame_height}")

            if result.boxes.id is not None: # If tracks are present
                boxes_xyxy = result.boxes.xyxy.cpu().numpy()
                track_ids = result.boxes.id.cpu().numpy().astype(int)
                class_ids = result.boxes.cls.cpu().numpy().astype(int)
                confidences = result.boxes.conf.cpu().numpy()

                for i in range(len(track_ids)):
                    box = boxes_xyxy[i]
                    track_id = track_ids[i]
                    cls_id = class_ids[i]
                    conf = confidences[i]
                    class_name = model.names[cls_id]

                    # --- Core Per-Class Counting Logic ---
                    per_class_counted_track_ids[class_name].add(track_id)

                    # --- Drawing detections and track IDs with Class-Specific Colors ---
                    label = f"ID:{track_id} {class_name} {conf:.2f}"
                    x1, y1, x2, y2 = map(int, box)

                    # Get color for the current class
                    # Use modulo operator to cycle through colors if more classes than defined colors
                    color = CLASS_COLORS[cls_id % len(CLASS_COLORS)]

                    # Draw bounding box with class-specific color
                    cv2.rectangle(frame_to_draw, (x1, y1), (x2, y2), color, 2)

                    # Draw label background with class-specific color
                    (w, h), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1) # Thickness 1 for text
                    cv2.rectangle(frame_to_draw, (x1, y1 - h - 5), (x1 + w, y1 - 5), color, -1)

                    # Draw label text (e.g., in black or white for contrast)
                    # Determine text color based on background color brightness for better visibility
                    text_color = (0,0,0) if sum(color) > 382 else (255,255,255) # If color is light, use black text, else white
                    cv2.putText(frame_to_draw, label, (x1, y1 - 5),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.5, text_color, 1, cv2.LINE_AA)

            # --- Display Per-Class Counts on the Frame ---
            text_y_offset = 30
            line_height = 25
            max_text_width = 0
            num_classes_to_display = len(per_class_counted_track_ids)

            if num_classes_to_display == 0 and frame_count_processed < 50:
                temp_text = "Detecting vehicles..."
                (w,h), _ = cv2.getTextSize(temp_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)
                max_text_width = w
                # Draw semi-transparent black background for the "Detecting vehicles..." text
                # Create a separate overlay for transparency
                overlay = frame_to_draw.copy()
                cv2.rectangle(overlay, (25, text_y_offset - line_height + 5),
                              (25 + max_text_width + 10, text_y_offset + 10),
                              (0,0,0), -1)
                alpha = 0.6 # Transparency factor
                cv2.addWeighted(overlay, alpha, frame_to_draw, 1 - alpha, 0, frame_to_draw)
                cv2.putText(frame_to_draw, temp_text, (30, text_y_offset),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)

            elif num_classes_to_display > 0 :
                for class_name_disp in sorted(per_class_counted_track_ids.keys()):
                    count_text = f"{class_name_disp}: {len(per_class_counted_track_ids[class_name_disp])}"
                    (w,h), _ = cv2.getTextSize(count_text, cv2.FONT_HERSHEY_SIMPLEX, 0.7, 2)
                    if w > max_text_width:
                        max_text_width = w

                if max_text_width > 0:
                    bg_y1 = text_y_offset - line_height + 10 # Adjusted for padding
                    bg_y2 = text_y_offset + (num_classes_to_display * line_height) - (line_height // 2) # Adjusted

                    overlay = frame_to_draw.copy()
                    cv2.rectangle(overlay, (25, bg_y1 - 5), # Slightly larger background
                                  (25 + max_text_width + 15, bg_y2 + 5),
                                  (0,0,0), -1)
                    alpha = 0.6 # Transparency factor
                    cv2.addWeighted(overlay, alpha, frame_to_draw, 1 - alpha, 0, frame_to_draw)


                current_y = text_y_offset + (line_height // 2) # Adjusted for better vertical centering
                for class_name_disp in sorted(per_class_counted_track_ids.keys()):
                    count = len(per_class_counted_track_ids[class_name_disp])
                    count_text = f"{class_name_disp}: {count}"
                    cv2.putText(frame_to_draw, count_text, (30, current_y),
                                cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2, cv2.LINE_AA)
                    current_y += line_height


            if video_writer:
                video_writer.write(frame_to_draw)

            frame_count_processed += 1
            if frame_count_processed % 100 == 0:
                print(f"  Processed {frame_count_processed} frames...")

    except Exception as e:
        print(f"An error occurred during video processing: {e}")
    finally:
        if video_writer:
            video_writer.release()
            print(f"\nVideo processing complete.")
            if os.path.exists(output_video_name_per_class) and os.path.getsize(output_video_name_per_class) > 0:
                print(f"Per-class counted video saved at: {output_video_name_per_class}")
            else:
                print(f"Output video file {output_video_name_per_class} not found or is empty.")
else:
    if not model:
        print("Model not loaded. Please load your 'best.pt' model first.")
    if not os.path.exists(path_to_your_test_video):
        print(f"Test video not found at {path_to_your_test_video}. Please check the path.")

#Evaluate detection accuracy

from ultralytics import YOLO


model = YOLO('/content/runs/detect/yolov8s_ua_detrac_run1/weights/best.pt') # Or your specific path
model = YOLO('yolov8n.pt')


metrics = model.val(data='/content/drive/MyDrive/ua_detrac_config.yaml')
print(metrics.box.map)    # mAP50-95
print(metrics.box.map50)  # mAP@.5
print(metrics.box.map75)  # mAP@.75
print(metrics.box.maps)   # A list containing mAP50-95, mAP@.5, mAP@.75, etc.
results_dict = metrics.results_dict # Detailed results per class
print(results_dict)